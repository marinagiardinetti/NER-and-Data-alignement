{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Recherche et alignement : outil de recherche iconographique dans la base de données Gallica, Joconde et Europeana\n",
        "\n",
        "Cet outil permet la récupération de données (textuelles et iconographiques) à partir de termes de recherche de la bases de données : Gallica, Joconde et Europeana\n",
        "\n",
        "*Document d'entrée* : mot-clé.\n",
        "\n",
        "*Documents de sortie* : Dans le dossier de travail, les fichiers de données ainsi que les fichiers téléchargés"
      ],
      "metadata": {
        "id": "DeNYERyGkyIF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2vKBGDIvjkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4efe4f44-43dc-4a98-8ef0-cb67843090dd",
        "cellView": "form",
        "collapsed": true
      },
      "source": [
        "#@markdown # Connexion du notebook à son compte Google Drive et signalement du dossier de travail :\n",
        "\n",
        "'''\n",
        "Google Colab notebook.\n",
        "Python == 3.7.11\n",
        "\n",
        "Marina Giardinetti\n",
        "2024\n",
        "'''\n",
        "\n",
        "## Installation des bibliothèques et connexion au compte Google Drive\n",
        "\n",
        "!pip install utils\n",
        "from google.colab import drive\n",
        "import json\n",
        "from openpyxl import load_workbook\n",
        "import pandas as pd\n",
        "!pip install soup2dict\n",
        "from soup2dict import convert\n",
        "import ssl\n",
        "import re\n",
        "import requests\n",
        "!pip install langdetect\n",
        "from langdetect.lang_detect_exception import LangDetectException\n",
        "import shutil\n",
        "!pip install xmltojson\n",
        "import xmltodict\n",
        "import urllib\n",
        "import urllib.request, urllib.error, urllib.parse\n",
        "import xmltojson\n",
        "from langdetect import detect\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import lxml\n",
        "from collections import OrderedDict\n",
        "from lxml import etree\n",
        "!pip install unidecode==1.2.0\n",
        "import unidecode\n",
        "import folium\n",
        "import unicodedata\n",
        "from urllib.error import HTTPError\n",
        "from re import match\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.request, urllib.error, urllib.parse\n",
        "import string\n",
        "import time\n",
        "\n",
        "\n",
        "!pip uninstall pyOpenSSL==0.15.1 -y\n",
        "!pip install pyOpenSSL==17.2.0\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "\n",
        "#@markdown - Lancer la cellule\n",
        "#@markdown - Cliquer sur « Exécuter malgré tout » lors de l’apparition du message d’avertissement indiquant que le notebook n’a pas été créé par Google\n",
        "#@markdown - Cliquer sur « Se connecter à Google Drive » lors de l’apparition du second message d’avertissement pour donner l’autorisation au notebook d’accéder à vos fichiers Google Drive\n",
        "#@markdown - Choisir son compte Gmail puis cliquer sur « Autoriser »\n",
        "\n",
        "#@markdown ####Indiquer le chemin vers le dossier de travail sur le Google Drive (si le dossier n'existe pas encore, il sera créé lors de l'exécution de la cellule) :\n",
        "chemin_vers_dossier_travail = '/content/drive/My Drive/BNF_15_05' # @param {type:\"string\"}\n",
        "\n",
        "if not os.path.exists(chemin_vers_dossier_travail):\n",
        "    os.makedirs(chemin_vers_dossier_travail)\n",
        "os.chdir(chemin_vers_dossier_travail)\n",
        "\n",
        "from urllib.error import HTTPError\n",
        "\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36'}\n",
        "terme_a_chercher = \"cleopatre\" # @param {type:\"string\"}\n",
        "os.chdir(chemin_vers_dossier_travail)\n",
        "os.makedirs(terme_a_chercher, exist_ok=True)\n",
        "os.chdir(terme_a_chercher)\n",
        "\n",
        "\n",
        "chemin_dossier = \"\".join(chemin_vers_dossier_travail+'/'+terme_a_chercher)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: utils in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: soup2dict in /usr/local/lib/python3.10/dist-packages (2.1.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.3 in /usr/local/lib/python3.10/dist-packages (from soup2dict) (4.12.3)\n",
            "Requirement already satisfied: classes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from soup2dict) (0.4.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.3->soup2dict) (2.5)\n",
            "Requirement already satisfied: typing_extensions<5.0,>=3.10 in /usr/local/lib/python3.10/dist-packages (from classes<0.5.0,>=0.4.0->soup2dict) (4.11.0)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Requirement already satisfied: xmltojson in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
            "Requirement already satisfied: xmltodict<0.13.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from xmltojson) (0.12.0)\n",
            "Requirement already satisfied: unidecode==1.2.0 in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Found existing installation: pyOpenSSL 17.2.0\n",
            "Uninstalling pyOpenSSL-17.2.0:\n",
            "  Successfully uninstalled pyOpenSSL-17.2.0\n",
            "Collecting pyOpenSSL==17.2.0\n",
            "  Using cached pyOpenSSL-17.2.0-py2.py3-none-any.whl (52 kB)\n",
            "Requirement already satisfied: cryptography>=1.9 in /usr/local/lib/python3.10/dist-packages (from pyOpenSSL==17.2.0) (42.0.7)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.10/dist-packages (from pyOpenSSL==17.2.0) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=1.9->pyOpenSSL==17.2.0) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=1.9->pyOpenSSL==17.2.0) (2.22)\n",
            "Installing collected packages: pyOpenSSL\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydrive2 1.6.3 requires pyOpenSSL>=19.1.0, but you have pyopenssl 17.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyOpenSSL-17.2.0\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Gallica\n",
        "\n",
        "os.chdir(chemin_dossier)\n",
        "dossier_base_gallica = 'images_gallica'\n",
        "os.makedirs(dossier_base_gallica, exist_ok=True)\n",
        "new_gallica = {}\n",
        "\n",
        "if ' ' in terme_a_chercher:\n",
        "  nom_pour_lien = re.sub(r' ', '%20', terme_a_chercher)\n",
        "  lien_de_recherche_gallica = \"\".join([\"https://gallica.bnf.fr/SRU?version=1.2&operation=searchRetrieve&query=dc.title%20adj%20\", nom_pour_lien, \"&maximumRecords=49&deway=%20any%207\"])\n",
        "else:\n",
        "  lien_de_recherche_gallica = \"\".join([\"https://gallica.bnf.fr/SRU?version=1.2&operation=searchRetrieve&query=dc.title%20adj%20\"+terme_a_chercher+'&maximumRecords=49&deway=%20any%207'])\n",
        "\n",
        "time.sleep(5)\n",
        "response = requests.get(lien_de_recherche_gallica, verify=False, headers=headers, timeout=5)\n",
        "\n",
        "parse = BeautifulSoup(response.content,'html.parser')\n",
        "with open ('fichier_temporaire.xml', 'w') as pu:\n",
        " pu.write(parse.prettify())\n",
        "with open('fichier_temporaire.xml', 'r') as po:\n",
        "  my =xmltodict.parse(po.read())\n",
        "  my2d = dict(OrderedDict(my))\n",
        "a_suppr = \"\".join(chemin_vers_dossier_travail+'/'+terme_a_chercher+'/fichier_temporaire.xml')\n",
        "os.remove(a_suppr)\n",
        "#my2 = xmltodict.parse(my2d)\n",
        "for ele in my2d['srw:searchretrieveresponse']['srw:records']['srw:record']:\n",
        "  for key, value in ele.items():\n",
        "    if key == 'srw:recorddata':\n",
        "      for key2, value2 in value.items():\n",
        "        for key3, value3 in value2.items():\n",
        "          if key3 == 'dc:title':\n",
        "            le_titre = str(value3)\n",
        "            le_nouveau_titre = re.sub(r\"[\\['\\]]\", '', le_titre)\n",
        "            new_gallica[le_nouveau_titre] = {}\n",
        "          if key3 == 'dc:date':\n",
        "            date = value3\n",
        "          if key3 == 'dc:creator':\n",
        "            auteur = value3\n",
        "          if key3 == 'dc:type':\n",
        "            types = value3\n",
        "          if key3 == 'dc:subject':\n",
        "            subject = value3\n",
        "          if key3 == 'dc:identifier':\n",
        "            if type(value3) == list:\n",
        "              value3 = value3[0]\n",
        "              identifiant = re.findall('b[a-z0-9]{4,}', value3)\n",
        "            else:\n",
        "              identifiant = re.findall('b[a-z0-9]{4,}', value3)\n",
        "            for id in identifiant:\n",
        "              lien = 'http://gallica.bnf.fr/iiif/ark:/12148/' + id + '/f1/full/3000/0/native.jpg'\n",
        "              nom_image = \"\".join([chemin_vers_dossier_travail+'/'+terme_a_chercher+ '/', dossier_base_gallica, '/', id + '.jpg'])\n",
        "              time.sleep(15)\n",
        "              r=requests.get(lien, headers=headers, stream=True)\n",
        "              with open(nom_image, 'wb') as f:\n",
        "                f.write(r.content)\n",
        "      try:\n",
        "        new_gallica[le_nouveau_titre] = {'Date': date, 'Auteur':auteur, 'Type': types, 'Sujet': subject, 'Lien': lien}\n",
        "      except NameError:\n",
        "        try:\n",
        "          auteur\n",
        "          subject\n",
        "          date\n",
        "          types\n",
        "        except NameError:\n",
        "          auteur = ''\n",
        "          subject = ''\n",
        "          date = ''\n",
        "          types = ''\n",
        "          new_gallica[le_nouveau_titre] = {'Date': date, 'Auteur':auteur, 'Type': types, 'Sujet': subject, 'Lien': lien}\n",
        "\n",
        "\n",
        "df_gallica = pd.DataFrame(dict([(key, pd.Series(value)) for key, value in new_gallica.items()]))\n",
        "df_gallica = df_gallica.transpose()\n",
        "\n",
        "df_gallica.to_excel('donnees_gallica.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "collapsed": true,
        "id": "dnYUOOohfIZl",
        "outputId": "fd53e689-a629-4e64-c7d3-7c2a3f0feeae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'gallica.bnf.fr'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/lib/python3.10/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "  k = self.parse_starttag(i)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Europeana\n",
        "from json.decoder import JSONDecodeError\n",
        "os.chdir(chemin_dossier)\n",
        "dossier_base_europeana = 'images_europeana'\n",
        "os.makedirs(dossier_base_europeana, exist_ok=True)\n",
        "new_europeana = {}\n",
        "\n",
        "if ' ' in terme_a_chercher:\n",
        "  nom_pour_lien = re.sub(r' ', '%20', terme_a_chercher)\n",
        "  lien_de_recherche_europeana = \"\".join([\"https://api.europeana.eu/record/v2/search.json?query=\"+nom_pour_lien+'&rows=100&theme=art&wskey=alemalle'])\n",
        "else:\n",
        "  lien_de_recherche_europeana = \"\".join([\"https://api.europeana.eu/record/v2/search.json?query=\"+terme_a_chercher+'&rows=100&theme=art&wskey=alemalle'])\n",
        "request = urllib.request.Request(lien_de_recherche_europeana)\n",
        "try:\n",
        "  content = urllib.request.urlopen(request)\n",
        "except HTTPError as he:\n",
        "  print('Pas de téléchargement possible pour la base Europeana avec cette recherche')\n",
        "parse = BeautifulSoup(content,'html.parser')\n",
        "json_infos=json.loads(parse.text)\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "for keys, values in json_infos.items():\n",
        "  if keys == 'items':\n",
        "    for ele in values:\n",
        "      for keys2, values2 in ele.items():\n",
        "        if keys2 == 'title':\n",
        "          if type(values2) == list:\n",
        "            titre_trouve = values2[0]\n",
        "          nouveau_titre_trouve = re.sub(r\"[\\['\\]]\",'',  titre_trouve)\n",
        "        if keys2 == 'dcCreator':\n",
        "            auteurs = str(values2)\n",
        "        if keys2 == 'edmConceptPrefLabelLangAware':\n",
        "          for keys3, values3 in values2.items():\n",
        "            if keys3 == 'fr':\n",
        "              typee = str(values3)\n",
        "        if keys2 == 'edmConceptLabel':\n",
        "          for ele2 in values2:\n",
        "            for keys4, values4 in ele2.items():\n",
        "              try:\n",
        "                lang = detect(values4)\n",
        "              except LangDetectException as lde:\n",
        "                pass\n",
        "              if lang == 'fr':\n",
        "                sujets = str(values4)\n",
        "        if keys2 == 'edmIsShownBy':\n",
        "          for ele2 in values2:\n",
        "            lien = ele2\n",
        "            nom = re.sub('/', '_', lien)\n",
        "            try:\n",
        "              titre_pour_nom = re.sub('\\/', '_', nouveau_titre_trouve)\n",
        "              nom_image2 = ''.join([chemin_vers_dossier_travail+'/'+terme_a_chercher+ '/', dossier_base_europeana, '/', titre_pour_nom + '.jpg'])\n",
        "              if 'gallica' in lien:\n",
        "                pass\n",
        "              else:\n",
        "                r=requests.get(lien, headers=headers, stream=True)\n",
        "                with open(nom_image2, 'wb') as f:\n",
        "                  f.write(r.content)\n",
        "         #   urllib.request.urlretrieve(lien, nom_image2)\n",
        "            except (NameError, requests.exceptions.SSLError, OSError) :pass\n",
        "      try:\n",
        "        new_europeana[nouveau_titre_trouve]= {'Auteur': auteurs, 'Type': typee, 'Sujet': sujets, 'Lien': lien}\n",
        "      except NameError:\n",
        "          auteurs = ''\n",
        "          new_europeana[nouveau_titre_trouve]= {'Auteur': auteurs, 'Type': typee, 'Sujet': sujets, 'Lien': lien}\n",
        "\n",
        "df_europeana = pd.DataFrame(dict([(key, pd.Series(value)) for key, value in new_europeana.items()]))\n",
        "df_europeana = df_europeana.transpose()\n",
        "\n",
        "df_europeana.to_excel('donnees_europeana.xlsx')"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "F-dprLTbfLzF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGvqf6C5K33D",
        "cellView": "form"
      },
      "source": [
        "#@markdown # Base Joconde\n",
        "os.chdir(chemin_dossier)\n",
        "new_joconde = {}\n",
        "auteurss, sujetss,materiaux_techniques, datess = ([] for i in range(4))\n",
        "\n",
        "if ' ' in terme_a_chercher:\n",
        "  nom_pour_lien = re.sub(r' ', '%20', terme_a_chercher)\n",
        "  lien_de_recherche_joconde = \"\".join(\"https://data.culture.gouv.fr/api/records/1.0/search/?dataset=base-joconde-extrait&q=\"+nom_pour_lien+'&rows=10000')\n",
        "else:\n",
        "  lien_de_recherche_joconde=\"\".join(\"https://data.culture.gouv.fr/api/records/1.0/search/?dataset=base-joconde-extrait&q=\"+terme_a_chercher+'&rows=10000')\n",
        "request = urllib.request.Request(lien_de_recherche_joconde)\n",
        "content = urllib.request.urlopen(request)\n",
        "parse = BeautifulSoup(content,'html.parser')\n",
        "donnees_joconde=json.loads(parse.text)\n",
        "\n",
        "for key, value in donnees_joconde.items():\n",
        "  if key == 'records':\n",
        "    for ele in value:\n",
        "      for key2, value2 in ele.items():\n",
        "        if key2 == 'fields':\n",
        "          for key3, value3 in value2.items():\n",
        "            if key3 == 'titre':\n",
        "              titre = str(value3)\n",
        "            if key3 == 'auteur':\n",
        "              auteurss = value3\n",
        "            if key3 == 'materiaux_techniques':\n",
        "              materiaux_techniques = value3\n",
        "            if key3 == 'sujet_represente':\n",
        "              sujetss = value3\n",
        "            if key3 == 'epoque':\n",
        "              datess = str(value3)\n",
        "            if key3 == 'periode_de_creation':\n",
        "              datess = value3\n",
        "      try:\n",
        "        new_joconde[titre]={'Date': datess, 'Auteur':auteurss, 'Matériaux': materiaux_techniques, 'Sujet': sujetss}\n",
        "      except NameError:pass\n",
        "\n",
        "df_joconde = pd.DataFrame(dict([(key, pd.Series(value)) for key, value in new_joconde.items()]))\n",
        "df_joconde = df_joconde.transpose()\n",
        "df_joconde.to_excel('donnees_joconde.xlsx')\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}